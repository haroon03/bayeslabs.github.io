In this post I will give an explanation of the code. 

<h3>ORGAN: MODEL</h3>
<b>Dataset:</b>

ChEMBL SMILES data-set has been used for the training purpose of the GAN. 
<b>Simplified Molecular-Input Line-Entry System (SMILES)</b>. SMILES is a line notation for representing molecules and reactions.

![previous](SMILES.png)

We load the Data-set and convert it into a string, Create a vocabulary of all the characters present in the data-set. The characters "\<bos>","\<eos>", "\<unk>" and "\<pad>" <b>(markers)</b> were added to the vocabulary.
     <li>"\<bos>": marks the beginning of sequence</li>
     <li>"\<eos>": marks end of sequences</li>
     <li>"\<unk>": specifies an unknown character</li>
     <li>"\<pad>": specifies padding</li>
Also the characters in vocabulary are indexed (c2i & i2c).We convert the smiles into tensors using the index.
Every time a smiles string is converted int tensor we add all the four markers at their specific locations. "bos" at the beginning of sequence, "eos" at the end of sequence, "unk" for characters unknown i.e. not in vocabulary. Padding is done to maintain a specific sequence length, 100 here.Now,we have the data that our computer can read and understand.

<b>Generator(G):</b> It is a Recurrent Neural Network(RNN) with Long-short Term Memory (LSTM) cells. It is responsible for generating molecules that closely follows the distribution of training data. A generator can be assumed as a money forger. The Generator is initially trained on the training set to generate molecules.
LSTM layers are best for large sequrntial data. They have better memory retention power than GRU.
It takes the initial character(tensor) from sequence and predicts the next one until "/<eos>".It outputs the sequence(x), its length(lengths), and the current state.

```python
class Generator(nn.Module):
    def __init__(self, embedding_layer, hidden_size, num_layers, dropout):
        super(Generator, self).__init__()

        self.embedding_layer = embedding_layer
        self.lstm_layer = nn.LSTM(embedding_layer.embedding_dim,
                                  hidden_size, num_layers,
                                  batch_first=True, dropout=dropout)
        self.linear_layer = nn.Linear(hidden_size,
                                      embedding_layer.num_embeddings)

    def forward(self, x, lengths, states=None):
        x = self.embedding_layer(x)
        x = pack_padded_sequence(x, lengths, batch_first=True)
        x, states = self.lstm_layer(x, states)
        x, _ = pad_packed_sequence(x, batch_first=True)
        x = self.linear_layer(x)

        return x, lengths, states
```
What we are doing here is, we initially use the embedding layer to understand the relationship between the characters and generate a tensor with dimension equla to embedding dimension.


<b>Discriminator(D):</b>It plays the role of a cop who is trained to catch fake molecules generated by G.The Discriminator is composed of Convolutional Neural Networks(CNN), specifically designed for text classification. It gives a probability estimation of the molecule of either being fake(0/generated) or real(1/belongs to true/training data). 

```python
class Discriminator(nn.Module):
    def __init__(self, desc_embedding_layer, convs, dropout=0):
        super(Discriminator, self).__init__()

        self.embedding_layer = desc_embedding_layer
        self.conv_layers = nn.ModuleList(
            [nn.Conv2d(1, f, kernel_size=(
                n, self.embedding_layer.embedding_dim)
                       ) for f, n in convs])
        sum_filters = sum([f for f, _ in convs])
        self.highway_layer = nn.Linear(sum_filters, sum_filters)
        self.dropout_layer = nn.Dropout(p=dropout)
        self.output_layer = nn.Linear(sum_filters, 1)

    def forward(self, x):
        x = self.embedding_layer(x)
        x = x.unsqueeze(1)
        convs = [F.elu(conv_layer(x)).squeeze(3)
                 for conv_layer in self.conv_layers]
        x = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]
        x = torch.cat(x, dim=1)

        h = self.highway_layer(x)
        t = torch.sigmoid(h)
        x = t * F.elu(h) + (1 - t) * x
        x = self.dropout_layer(x)
        out = self.output_layer(x)

        return out
```



<h3>Training</h3> 
As discussed in previous post training has 2 phases.
Here we begin training the generator initially with training data.Foreach epoch the sequence is split into previous and next.  previous is sequence[:-1] and next is sequence[1:]. The generator presicts the next character and each time loss is calculated and the gerrator is optimsed and parameters updated.
While training the Generator the discriminator is kept in eval mode(freezed).

```python
def _pretrain_generator_epoch(model, tqdm_data, criterion, optimizer):
    model.discriminator.eval()
    if optimizer is None:
        model.eval()
    else:
        model.train()

    postfix = {'loss': 0, 'running_loss': 0}

    for i, batch in enumerate(tqdm_data):
        (prevs, nexts, lens) = (data.to(device) for data in batch)
        outputs, _, _, = model.generator_forward(prevs, lens)
        loss = criterion(outputs.view(-1, outputs.shape[-1]),nexts.view(-1))
        if optimizer is not None:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        postfix['loss'] = loss.item()
        postfix['running_loss'] += (loss.item() - postfix['running_loss']) / (i + 1)
        tqdm_data.set_postfix(postfix)

    postfix['mode'] = ('Pretrain: eval generator'
                       if optimizer is None
                       else 'Pretrain: train generator')
    return postfix
```      

Now we freeze the generator and train the discriminator. The training process of discriminator comprises of 2 parts. It is trained first on the training data with labels. Since the Data set has valid molecules all are labelled 1.
Loss is calculated for each prediction and the 
 
Simultaneously it is also trained on fake data generated by the generator, all labelled 0. Loss is calculated, generator is optimzed and all parameters updated.

```python
ef _pretrain_discriminator_epoch(model, tqdm_data,
                                  criterion, optimizer=None):
    model.eval()
    if optimizer is None:
        model.eval()
    else:
        model.train()

    postfix = {'loss': 0,
               'running_loss': 0}
    for i, inputs_from_data in enumerate(tqdm_data):
        inputs_from_data = inputs_from_data.to(device)
        inputs_from_model, _ = model.sample_tensor(n_batch, 100)

        targets = torch.zeros(n_batch, 1, device=device)
        outputs = model.discriminator_forward(inputs_from_model)
        loss = criterion(outputs, targets) / 2

        targets = torch.ones(inputs_from_data.shape[0], 1, device=device)
        outputs = model.discriminator_forward(inputs_from_data)
        loss += criterion(outputs, targets) / 2

        if optimizer is not None:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        postfix['loss'] = loss.item()
        postfix['running_loss'] += (loss.item() -
                                    postfix['running_loss']) / (i + 1)
        tqdm_data.set_postfix(postfix)

    postfix['mode'] = ('Pretrain: eval discriminator'
                       if optimizer is None
                       else 'Pretrain: train discriminator')
    return postfix
```



