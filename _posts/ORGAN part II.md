In this post I will give an explanation of the code. 

<h3>ORGAN: MODEL</h3>
<b>Dataset</b>
ChEMBL SMILES data-set has been used for the training purpose of the GAN. 
<b>Simplified Molecular-Input Line-Entry System (SMILES)</b>. SMILES is a line notation for representing molecules and reactions.

{% include image.html url="/assets/img/SMILES.png" description="SMILES" %}

We load the Data-set and convert it into a string, Create a vocabulary of all the characters present in the data-set. The characters "\<bos>","\<eos>", "\<unk>" and "\<pad>" <b>(markers)</b> were added to the vocabulary.
    <li>
     <li>"\<bos>": marks the beginning of sequence</li>
     <li>"\<eos>": marks end of sequences</li>
     <li>"\<unk>": specifies an unknown character</li>
     <li>"\<pad>": specifies padding
    </li>
Also the characters in vocabulary are indexed (c2i & i2c).We convert the smiles into tensors using the index.
Every time a smiles string is converted int tensor we add all the four markers at their specific locations. "bos" at the beginning of sequence, "eos" at the end of sequence, "unk" for characters unknown i.e. not in vocabulary. Padding is done to maintain a specific sequence length, 100 here.

<b>Generator(G):</b> It is a Recurrent Neural Network(RNN) with Long-short Term Memory (LSTM) cells. It is responsible for generating molecules that closely follows the distribution of training data. A generator can be assumed as a money forger. The Generator is initially trained on the training set using Maximum Likelihood Estimation(MLE) to generate molecules.
It takes the initial vector and predicts the next one untill

```python
class Generator(nn.Module):
    def __init__(self, embedding_layer, hidden_size, num_layers, dropout):
        super(Generator, self).__init__()

        self.embedding_layer = embedding_layer
        self.lstm_layer = nn.LSTM(embedding_layer.embedding_dim,
                                  hidden_size, num_layers,
                                  batch_first=True, dropout=dropout)
        self.linear_layer = nn.Linear(hidden_size,
                                      embedding_layer.num_embeddings)

    def forward(self, x, lengths, states=None):
        x = self.embedding_layer(x)
        x = pack_padded_sequence(x, lengths, batch_first=True)
        x, states = self.lstm_layer(x, states)
        x, _ = pad_packed_sequence(x, batch_first=True)
        x = self.linear_layer(x)

        return x, lengths, states
```

<b>Discriminator(D):</b>It plays the role of a cop who is trained to catch fake molecules generated by G.The Discriminator is composed of Convolutional Neural Networks(CNN), specifically designed for text classification. It gives a probability estimation of the molecule of either being fake(0/generated) or real(1/belongs to true/training data). 

```python
class Discriminator(nn.Module):
    def __init__(self, desc_embedding_layer, convs, dropout=0):
        super(Discriminator, self).__init__()

        self.embedding_layer = desc_embedding_layer
        self.conv_layers = nn.ModuleList(
            [nn.Conv2d(1, f, kernel_size=(
                n, self.embedding_layer.embedding_dim)
                       ) for f, n in convs])
        sum_filters = sum([f for f, _ in convs])
        self.highway_layer = nn.Linear(sum_filters, sum_filters)
        self.dropout_layer = nn.Dropout(p=dropout)
        self.output_layer = nn.Linear(sum_filters, 1)

    def forward(self, x):
        x = self.embedding_layer(x)
        x = x.unsqueeze(1)
        convs = [F.elu(conv_layer(x)).squeeze(3)
                 for conv_layer in self.conv_layers]
        x = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]
        x = torch.cat(x, dim=1)

        h = self.highway_layer(x)
        t = torch.sigmoid(h)
        x = t * F.elu(h) + (1 - t) * x
        x = self.dropout_layer(x)
        out = self.output_layer(x)

        return out

```

<b>Reinforcement Metric:</b> Molecular metrics are implemented using the RDKit chem-informatics package. Metrics include Synthesis Accessibility, Natural Product likeliness, Drug-likeness, LogP. These were applied to calculate the reward for each generated molecule. Reinforcement provides a quality metric (between 0 & 1) which gives the desirability of a specific molecule, where 1 being highly desirable and 0 being highly undesirable.

The main objective of the reinforcement metric is to maximize the reward by optimizing the generator to generate molecules similar to the initial distribution of data. The molecules generated are then analyzed by the discriminator and the reward metric, which then optimize or train the generator to fool the discriminator.

<h3>Dataset:</h3> 
ChEMBL SMILES dataset has been used for the training purpose of the GAN. <b>Simplified Molecular-Input Line-Entry System (SMILES)</b>. SMILES is a line notation for representing molecules and reactions.
