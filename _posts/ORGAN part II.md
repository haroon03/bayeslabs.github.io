In this post I will give an explanation of the code. 

<h3>ORGAN: MODEL</h3>
<b>Dataset</b>
ChEMBL SMILES data-set has been used for the training purpose of the GAN. 
<b>Simplified Molecular-Input Line-Entry System (SMILES)</b>. SMILES is a line notation for representing molecules and reactions.

{% include image.html url="/assets/img/SMILES.png" description="SMILES" %}

We load the Data-set and convert it into a string, Create a vocabulary of all the characters present in the data-set. The characters "\<bos>","\<eos>", "\<unk>" and "\<pad>" <b>(markers)</b> were added to the vocabulary.
     <li>"\<bos>": marks the beginning of sequence</li>
     <li>"\<eos>": marks end of sequences</li>
     <li>"\<unk>": specifies an unknown character</li>
     <li>"\<pad>": specifies padding</li>
Also the characters in vocabulary are indexed (c2i & i2c).We convert the smiles into tensors using the index.
Every time a smiles string is converted int tensor we add all the four markers at their specific locations. "bos" at the beginning of sequence, "eos" at the end of sequence, "unk" for characters unknown i.e. not in vocabulary. Padding is done to maintain a specific sequence length, 100 here.Now,we have the data that our computer can read and understand.

<b>Generator(G):</b> It is a Recurrent Neural Network(RNN) with Long-short Term Memory (LSTM) cells. It is responsible for generating molecules that closely follows the distribution of training data. A generator can be assumed as a money forger. The Generator is initially trained on the training set to generate molecules.
LSTM layers are best for large sequrntial data. They have better memory retention power than GRU.
It takes the initial character(tensor) from sequence and predicts the next one until "/<eos>".It outputs the sequence(x), its length(lengths), and the current state.

```python
class Generator(nn.Module):
    def __init__(self, embedding_layer, hidden_size, num_layers, dropout):
        super(Generator, self).__init__()

        self.embedding_layer = embedding_layer
        self.lstm_layer = nn.LSTM(embedding_layer.embedding_dim,
                                  hidden_size, num_layers,
                                  batch_first=True, dropout=dropout)
        self.linear_layer = nn.Linear(hidden_size,
                                      embedding_layer.num_embeddings)

    def forward(self, x, lengths, states=None):
        x = self.embedding_layer(x)
        x = pack_padded_sequence(x, lengths, batch_first=True)
        x, states = self.lstm_layer(x, states)
        x, _ = pad_packed_sequence(x, batch_first=True)
        x = self.linear_layer(x)

        return x, lengths, states
```
What we are doing here is, we initially use the embedding layer to understand the relationship between the characters and generate a tensor with dimension equla to embedding dimension.


<b>Discriminator(D):</b>It plays the role of a cop who is trained to catch fake molecules generated by G.The Discriminator is composed of Convolutional Neural Networks(CNN), specifically designed for text classification. It gives a probability estimation of the molecule of either being fake(0/generated) or real(1/belongs to true/training data). 

```python
class Discriminator(nn.Module):
    def __init__(self, desc_embedding_layer, convs, dropout=0):
        super(Discriminator, self).__init__()

        self.embedding_layer = desc_embedding_layer
        self.conv_layers = nn.ModuleList(
            [nn.Conv2d(1, f, kernel_size=(
                n, self.embedding_layer.embedding_dim)
                       ) for f, n in convs])
        sum_filters = sum([f for f, _ in convs])
        self.highway_layer = nn.Linear(sum_filters, sum_filters)
        self.dropout_layer = nn.Dropout(p=dropout)
        self.output_layer = nn.Linear(sum_filters, 1)

    def forward(self, x):
        x = self.embedding_layer(x)
        x = x.unsqueeze(1)
        convs = [F.elu(conv_layer(x)).squeeze(3)
                 for conv_layer in self.conv_layers]
        x = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in convs]
        x = torch.cat(x, dim=1)

        h = self.highway_layer(x)
        t = torch.sigmoid(h)
        x = t * F.elu(h) + (1 - t) * x
        x = self.dropout_layer(x)
        out = self.output_layer(x)

        return out
```

<b>Reinforcement Metric:</b> Molecular metrics are implemented using the RDKit chem-informatics package. Metrics include Synthesis Accessibility, Natural Product likeliness, Drug-likeness, LogP, Nearest Neighbour Similarity. These were applied to calculate the reward for each generated molecule. Reinforcement provides a quality metric (between 0 & 1) which gives the desirability of a specific molecule, where 1 being highly desirable and 0 being highly undesirable.

The main objective of the reinforcement metric is to maximize the reward by optimizing the generator to generate molecules similar to the initial distribution of data. The molecules generated are then analyzed by the discriminator and the reward metric, which then optimize or train the generator to fool the discriminator.

<h3>Training</h3> 
As discussed in previous post training has 2 phases.
Here we begin training the generator initially with training data. The sequence is split into previous and next.  previous is sequence[:-1] and next is sequence[1:]. The generator presicts the next character and each time loss is calculated and the gerrator is optimsed and parameters updated.


```python
def _pretrain_generator_epoch(model, tqdm_data, criterion, optimizer):
    model.discriminator.eval()
    if optimizer is None:
        model.eval()
    else:
        model.train()

    postfix = {'loss': 0, 'running_loss': 0}

    for i, batch in enumerate(tqdm_data):
        (prevs, nexts, lens) = (data.to(device) for data in batch)
        outputs, _, _, = model.generator_forward(prevs, lens)

        loss = criterion(outputs.view(-1, outputs.shape[-1]),
                         nexts.view(-1))

        if optimizer is not None:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        postfix['loss'] = loss.item()
        postfix['running_loss'] += (
                                           loss.item() - postfix['running_loss']
                                   ) / (i + 1)
        tqdm_data.set_postfix(postfix)

    postfix['mode'] = ('Pretrain: eval generator'
                       if optimizer is None
                       else 'Pretrain: train generator')
    return postfix


def _pretrain_generator(model, train_loader):
    #generator = model.generator
    criterion = nn.CrossEntropyLoss(ignore_index=c2i['<pad>'])
    optimizer = torch.optim.Adam(model.generator.parameters(), lr=1e-4)

    model.zero_grad()
    for epoch in range(generator_pretrain_epochs):
        tqdm_data = tqdm(train_loader, desc='Generator training (epoch #{})'.format(epoch))
        postfix = _pretrain_generator_epoch(model, tqdm_data, criterion, optimizer)
        if epoch % save_frequency == 0:
            generator = generator.to('cpu')
            torch.save(generator.state_dict(), 'model.csv'[:-4] +
                       '_generator_{0:03d}.csv'.format(epoch))
        generator = generator.to(device)
```        
