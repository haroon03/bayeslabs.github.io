---
layout: post
title:  "Objective Reinforced Generative Adversarial Network (Part I)"
date: 2019-07-03  
comments: True
mathjax: True
---

<h2>Introduction</h2>
<b>Objective-Reinforced Generative Adversarial Network (ORGAN)</b> is a modified version of a basic Generative Adversarial Network (GAN). 
Before we dig deep into theory and implementation of ORGAN let me brief you about the basics of GAN. 
A simple GAN is composed of two neural networks, Generator, and the Discriminator.

<li><b> Generator(G):</b></li> The main aim of the Generator is to generate fake samples which resemble the true data/distribution so closely, that the discriminator cannot differentiate between the true data and the fake ones, in other words, it tries to fool the discriminator. 

<li><b>Discriminator(D):</b></li> As the name suggests, it discriminates the input data and classifies whether it is from the true data sample/distribution or is a fake sample generated by the Generator(G). The Discriminator(D) is initially trained on true labeled data samples.

 {%include image.html url="\assets\img\GAN.png" dscription=" GAN Network"%}

Both of these networks work against each other trying to be better at their job by proving the other wrong. Their main objective is to 
generate data points that are similar to some data points in the training data;
Given an initial  training distribution p<sub>data</sub>, the generator G samples x from a distribution p<sub>synth</sub>, generated with random noise z, while a discriminator D looks at samples, either from p(synthetic) or from p<sub>data</sub>, and tries to classify their identity (y) as either real x∈p<sub>data</sub> or fake x∈p<sub>synth</sub>.

The model follows a min-max game where we minimize the Generator function log(1−D(G(z)) so that we can fool the discriminator by generating the samples very close to the original distribution, while maximizing the discriminator function log(D(x)) so that it can classify between fake and real data pints more accurately. 
<ul>
  <li>For a single data point we have: </li>
        $min_G  max_D  [\log D(x)] + z∼ p_{synthetic z} [ \log (1−D(G(z)))]$.
  <li>For the complete distributions we have: </li>
       $min_G max_D \mathrm{E}_{x ~ pdata} [ \log D(x) ] + \mathrm{E}_{z ~ psynth} [ \log (1-D(G(z)))]$.
 where E is Expectation.
</ul>
<h3>Training a GAN</h3>
Training a GAN is still a topic of research. Various problems have limited the power of GAN and its stability. Stability of GAN while training is also a major roadblock. If you start to train a GAN, and the discriminator part is much powerful than its generator counterpart, the generator would fail to train effectively. This will, in turn, affect the training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless. 

The training has two phases.
<ol>
  <li><b>Discriminator Training</b></li>
  We train the Discriminator on the labeled Training set for a certain epoch range. It must be trained well enough that it can discriminate the training data correctly as real(1). This is achieved by varying the number of epochs.
  While training Discriminator, the Generator is in freeze mode(freezing means setting training as false. The network does only forward pass and no back-propagation is applied).
  Afterward, we generate fake data and train the discriminator on it as well, until it predicts efficiently.
  Calculate the loss and optimize the network parameters and update the gradients.
  <li><b>Generator Training</b></li>
  Now to train the Generator, we use the predictions of discriminator as an objective to train the Generator.
  Similar to the previous training step we have discriminator in freeze mode while training the generator.
  Calculate the loss and optimize the network parameters and update the gradients.
</ol>

{%include image.html align = "center" url="\assets\img\Train.png" description="Training of a GAN" %}

This was a brief introduction to GANS's. Now, moving on to ORGAN, let's see what makes an ORGAN different from a GAN.
In ORGAN the main difference is the application of <b>Reinforcement Learning(RL) </b> to train the generator in a manner that it generates output with desired properties.
In ORGAN we bypass the generator differentiation problem by treating the specific discrete sequences as stochastic policy in an RL generator gradients setup. In other words, we update the generator parameters with policy gradient.

<h3>Reinforcement Learning</h3>

We treat the Generator as an agent here in an RL environment. We have <i><b>s</b></i> as the states with a reward function <i><b>Q</b></i>, <i><b>a</b></i> is the action that the agent chooses from action space <i><b>A</b></i> available in state <i><b>s</b></i>. The action space <i><b>A</b></i> composes of all the possible characters to select for the next character $x_{t+1}$. State s<sub>t</sub> is an already generated partial sequence of characters $X_{1:t}$. <i><b>Q(s,a)</b></i> is the action-value function that represents the expected reward at state <i><b>s</b></i> of taking action <i><b>a</b></i> and following our current policy to complete the rest of the sequence. When we are in state <i><b>s</b></i> we estimate <i><b>Q</b></i> value for every possible action, then we choose the action with the highest <i><b>Q</b></i> value. Let <i><b>R(X<sub>1: T</sub>)</b></i> be the reward function defined for full length sequences. Now, if we have an incomplete sequence $X_{1:t}$, in state <i><b>s</b></i> then , the generator $G_{\theta}$ (read G parametrized by $\theta$) must produce an action <i><b>a</b></i> with the next token $x_{t+1}$.
The agent's stochastic policy is given by $G(y_t | Y_{1:t-1})$ and our aim is to maximize the expected long-term reward $J_{\theta}$. 

$J(\theta)=\mathrm{E} [R_T \| s_{\theta}, \theta]=\sum_{x_1 \in X} G_{\theta}(x_1 \| s_{\theta}).Q(s_{\theta},x_1 )$.

The reward for generated molecules is calculated by reward metrics for specific properties. Some examples include LogP, Synthetic Acessibility, Natural Product-Likeness, Chemical Beauty(Quantitative Estimation of Drug-Likeness), Tanimoto Similarity, Nearest Neighbour Similarity.

{%include image.html align = "center" url="\assets\img\RL.png" description="Reinforcent Learning" %}

<b>Reinforcement Metric:</b> 
Molecular metrics are implemented using the RDKit chem-informatics package. Metrics include Synthetic Accessibility, Natural Product likeliness, Drug-likeness, LogP, Nearest Neighbour Similarity. These were applied to calculate the reward for each generated molecule. Reinforcement provides a quality metric (between 0 & 1) which gives the desirability of a specific molecule, 1 being highly desirable and 0 being highly undesirable.

The main objective of the reinforcement metric is to maximize the reward by optimizing the generator to generate molecules similar to the initial distribution of data. The molecules generated are then analyzed by the discriminator and the reward metric, which then optimize or train the generator to fool the discriminator.



We have completed the first half of the training. The above steps are called pretraining.
Now we train again both generator and discriminator but with a policy gradient. Since the generator has been trained let it generate molecules of its own by only providing the initial character "\<bos>".
For each character generated, the loss is calculated and the model is updated.
In case of Generator, policy gradient loss is calculated. The generator is then optimized and all parameters are updated. 

<h3> Policy Gradient Method</h3>
We start with a random arbitrary policy and go through some actions and if the rewards are better than expected, increase the  probability of those actions. If the rewards are worse we decrease the probability of taking those actions.

<b>Policy Function</b>
The policy function calculates the LogSoftmax of the output sequence given rewards, the targets, and the length of sequence. Its output is negative since we want to minimize loss but maximize the policy gradients.
The Policy gradient loss function looks like: 

$ L = -Q(s, a) \log (G(y_t \| Y_{1:t-1}))$.


Where $ Q(s, a)$ expected reward for an action <i><b>a</b></i> in state <i><b>s</b></i> and  $ G(y_t \| Y_{1:t-1})$ is the policy.

To get a more detailed explanation of policy gradient refer to this <a href="https://towardsdatascience.com/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c">blog</a>.


In the next <a href = "https://haroon03.github.io/2019/07/04/ORGAN-Part-II.html">blog</a> we will implement the above discussed theories to generate a desired molecule
