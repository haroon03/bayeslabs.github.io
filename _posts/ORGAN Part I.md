
<h2>Introduction</h2>
<b>Objective-Reinforced Generative Adversarial Network (ORGAN)</b> is a modified version of a basic Generative Adversarial Network (GAN). 
Before we dig deep into theory and implementation of ORGAN let me brief you about the basics of GAN. 
A simple GAN is composed of two neural networks,Generator, and the Discriminator.

<li><b> Generator:</b></li> The main aim of the Generator<b>(G)</b> is to produce/generate fake samples which resemble the true data/distribution so closely, that the discriminator cannot differentiate between the true data and the fake ones, in other words, it tries to fool the discriminator.

<li><b>Discriminator:</b></li> The Discriminator<b>(D)</b>, as the name suggests discriminates the input data and classifies whether it is from the true data sample/distribution or is a fake sample generated by the Generator(G). The Discriminator is initially trained on true labeled data samples.

Both of these networks work against each other trying to be better at their job by proving the other wrong. Their main objective is to 
generate data points that are similar to some data points in the training data;
Given an initial  training distribution p<sub>data</sub>, the generator G samples x from a distribution p<sub>synth</sub> generated with random noise z, while a discriminator D looks at samples, either from p(synthetic) or from p<sub>data</sub>, and tries to classify their identity (y) as either real x∈p<sub>data</sub> or fake x∈p<sub>synth</sub>.

The model follows a min-max game where we minimize the Generator function log(1−D(G(z)) so that we can fool the discriminator by generating the samples very close to the original distribution, while maximizing the discriminator function log(D(x)) so that it can classify between fake and real data pints more accurately. 
<ul>
  <li>For a single data point we have: </li>
        min<sub>G</sub> max<sub>D</sub> [logD(x)]+z∼p<sub>synthetic<sup>z</sup></sub>[log(1−D(G(z)))].
  <li>For the complete distributions we have: </li>
        min<sub>G</sub> max<sub>D</sub> E<sub>p<sub>data</sub></sub>[logD(x)] +E<sub>p<sub>synth</sub></sub>[log(1−D(G(z)))] where E is Expectation.
</ul>
<h3>Training a GAN</h3>
Training a GAN is still a topic of research. Various problems have limited the power of GAN and its stability. Stability of GAN while training is also a major roadblock. If you start to train a GAN, and the discriminator part is much powerful than its generator counterpart, the generator would fail to train effectively. This will, in turn, affect the training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless. 

The training has broadly two phases.
<ol>
  <li><b>Discriminator Training</b></li>
  We train the Discriminator on the labeled Training set for a certain epoch range. It must be trained well enough that it can discriminate the training data correctly as real(1). This is achieved by varying the number of epochs.
  While training Discriminator, the Generator is in freeze mode(freezing means setting training as false. The network does only forward pass and no back-propagation is applied).
  Afterward, we generate fake data and train the discriminator on it as well, until it predicts efficiently.
  Calculate the loss and optimize the network parameters and update the gradients.
  <li><b>Generator Training</b></li>
  Now to train the Discriminator we use the predictions of discriminator as an objective to train the Generator.
  Similar to the Discriminator training step we have discriminator in freeze mode while training the generator.
  Calculate the loss and optimize the network parameters and update the gradients.
</ol>

This was a brief introduction to GANS's. Now, moving on to ORGAN let see what makes ORGAN different from a GAN.
In ORGAN the main difference is the application of <b>Reinforcement Learning(RL) </b> to train the generator in a manner that it generates output with desired properties.
In ORGAN we bypass the generator differentiation problem by treating the specific discrete sequences as stochastic policy in an RL setup.
<h3>Reinforcement Learning</h3>
We treat the Generator as an agent here in an RL environment. We have <i><b>s</b></i> as the states with a reward function <i><b>Q</b></i>, <i><b>a</b></i> is the action that the agent chooses from action space <i><b>A</b></i> available in <i><b>s</b></i>. The action space <i><b>A</b></i> composes of all the possible characters to select for the next character x<sub>t+1</sub>. State s<sub>t</sub> is an already generated partial sequence of characters X<sub>1:t</sub>. <i><b>Q(s,a)</b></i> is the action-value function that represents the expected reward at state <i><b>s</b></i> of taking action <i><b>a</b></i> and following our current policy to complete the rest of the sequence. When we are in state <i><b>s</b></i> we estimate <i><b>Q</b></i> value for every possible action, then we choose the action with highest <i><b>Q</b></i> value. Now, if we have an incomplete sequence X<sub>1:t</sub>, in state <i><b>s</b></i> then , the generator G<sub>θ</sub> (read G parametrized by θ) must produce an action <i><b>a</b></i> with the next token x<sub>t+1</sub>.
The agent's stochastic policy is given by G(y<sub>t</sub>|Y<sub>1:t-1</sub>) and our aim is to maximize the expected long-term reward <i><b>J<sub>θ</sub></b></i>.
J(θ) =E[R<sub>T</sub>|s<sub>θ</sub>,θ]=∑<sub>x∈XGθ</sub>(x<sub>1</sub>|s<sub>θ</sub>)·Q(s<sub>θ</sub>,x<sub>1</sub>)



