
<h2>Introduction</h2>
<b>Objective-Reinforced Generative Adversarial Network (ORGAN)</b> is a modified version of a basic Generative Adversarial Network (GAN). 
Before we dig deep into theory and implementation of ORGAN let me brief you about the basics of GAN. 
A simple GAN is composed of two neural networks,Generator and the Discriminator.

<b> Generator:</b> The main aim of the Generator<b>(G)</b> is to produce/generate fake samples which resemble the true data/distribution so closely, that the discriminator cannot differentiate between the true data and the fake ones, in other words, it tries to fool the discriminator. 

<b>Discriminator:</b> The Discriminator<b>(D)</b>, as the name suggests discriminates the input data and classifies whether it is from the true data sample/distribution or is a fake sample generated by the Generator(G). The Discriminator is initially trained on true labeled data samples.

Both of these networks work against each other trying to be better at their job by proving the other wrong. Their main objective is to 
generate data points that are similar to some data points in the training data;
Given an initial  training distribution p(data), the generator G samples x from a distribution p(synth) generated with random noise z, while a discriminator D looks at samples, either from p(syntetic) or from p(data), and tries to classify their identity (y) as either real x∈p(data) or fake x∈p(synth).

The model follows a min max game where we minimize the Generator function log(1−D(G(z)) so that we can fool the discriminator by generating the samples vesry close to the original distribution, while maximizing the discriminator function logi(D(x)) so that it can classify beteen
fake and real data pints more accurately. 
For a single data point we have: 
min G max D [logD(x)]+z∼p(synthetic)(z)[log(1−D(G(z)))].
For the complete distributions we have: 
min G max D E[logD(x)] +E[log(1−D(G(z)))] where E is Expectation.

<h3>Training a GAN</h3>
Training a GAN is still a topic of research. Various problems have limited the power of GAN and its stability. Stability of GAN while training is also a major roadblock. If you start to train a GAN, and the discriminator part is much powerful than its generator counterpart, the generator would fail to train effectively. This will, in turn, affect the training of your GAN. On the other hand, if the discriminator is too lenient; it would let literally any image be generated. And this will mean that your GAN is useless. 

The training has broadly two phases.
<ol>
  <li><b>Discriminator Training</b></li>
  We train the Discriminator on the labeled Training set for a certain epoch range. It must be trained well enough that it can discriminate the training data correctly as real(1). This is achieved by varying the number of epochs.
  While training Discriminator the Generator is in freeze mode(Not in Training Mode).
  <li>Pr</li>

</ol>



Loss is calculated using Cross Entropy Loss method, gradients are updated via backpropagation. 

