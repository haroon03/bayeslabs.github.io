Objective-Reinforced Generative Adversarial Network (ORGAN) is a modified version of a basic Generative Adversarial Network (GAN). 
Before we dig deep into theory and implementation of ORGAN let me brief you about the basics of GAN.
A simple GAN is composed of two neural networks, The Generator and The Discriminator. 
(i). Generator: The main aim of the Generator is to produce/generate fake samples which resemble the true data/distribution so closely,
that the discriminator cannot differentiate between the true data and the fake ones, in other words, it tries to fool the discriminator.
(ii). Discriminator: The Discriminator(D), as the name suggests discriminates the input data and classifies whether it is from the true
data sample/distribution or is a fake sample generated by the Generator(G). The Discriminator is initially trained on true labeled data 
samples.

Both of these networks work against each other trying to be better at their job by proving the other wrong. Their main objective is to 
produce data points that are similar to some data points in the training data.An ORGAN takes into account the domain-specific desired 
objectives defined. This is achieved by using a reward function for each generated molecule. 

Components that make an ORGAN:
Generator(G): It is a Recurrent Neural Network(RNN) with Long-short Term Memory (LSTM) cells. It is responsible for generating molecules 
that closely follows the distribution of training data. A generator can be assumed as a money forger. The Generator is initially trained 
on the training set using Maximum Likelihood Estimation(MLE) to generate molecules.

Discriminator(D): It plays the role of a cop who is trained to catch fake molecules generated by G.The Discriminator is composed of 
Convolutional Neural Networks(CNN), specifically designed for text classification. It gives a probability estimation of the molecule of 
either being fake(0/generated) or real(1/belongs to true/training data). 

Reinforcement Metric: Molecular metrics are implemented using the RDKit chem-informatics package. Metrics include Synthesis Accessibility, 
Natural Product likeliness, Drug-likeness, LogP. These were applied to calculate the reward for each generated molecule. Reinforcement 
provides a quality metric (between 0 & 1) which gives the desirability of a specific molecule, where 1 being highly desirable and 0 being 
highly undesirable.

The main objective of the reinforcement metric is to maximize the reward by optimizing the generator to generate molecules similar to the 
initial distribution of data. The molecules generates are then analyzed by the discriminator and the reward metric, which then optimize or 
train the generator to fool the discriminator.
Both the generator and discriminator act as adversaries for each other, the main idea is that two different neural networks play a game 
against each other: given an initial training distribution p(data), the generator G samples x from a distribution p(synth) generated with 
random noise z, while a discriminator D looks at samples, either from p(syntetic) or from p(data), and tries to classify their identity (y)
as either real x∈p(data) or fake x∈p(synth).
Dataset: ChEMBL SMILES dataset has been used for the training purpose of the GAN. SMILES is a line notation for representing molecules 
and reactions.

min G max D Ex∼p(data)(x)[logD(x)] +Ez∼p(synthetic)(z)[log(1−D(G(z)))]
```python
import torch

```
